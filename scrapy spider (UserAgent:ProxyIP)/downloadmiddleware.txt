from scrapy import signals
from faker import Faker
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware


import sys
import time
import hashlib
# import requests
# import grequests
# from lxml import etree

#  随机useragent

class SpbeenDownloadmiddlewareRandomUseragent(object):
    def __init__(self):
        self.fake = Faker()

    def process_request(self,request,spider):
        # print(self.fake.user_agent())
        request.headers.setdefault('User-Agent',self.fake.user_agent())

class SpbeenDownloadmiddlewareRandomUseragent2(object):
    def __init__(self, user_agent):
        self.user_agent = user_agent

    @classmethod
    def from_crawler(cls, crawler):
        o = cls(Faker())
        # crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o



    def process_request(self, request, spider):
        # print(self.fake.user_agent())
        request.headers.setdefault('User-Agent', self.user_agent.user_agent())


#  代理IP使用

class SpbeenDownloadmiddlewareRandomProxyIP(object):
    def __init__(self):
	#  讯代理动态转发
        orderno = "ZF20193256314TXQ49I"
        secret = "3419379a8579425d836bce255cda5ed6"
        ip_port = "forward.xdaili.cn:80"
        timestamp = str(int(time.time()))  # 计算时间戳
        string = "orderno=" + orderno + "," + "secret=" + secret + "," + "timestamp=" + timestamp
        string = string.encode()
        md5_string = hashlib.md5(string).hexdigest()  # 计算sign
        sign = md5_string.upper()  # 转换成大写
        self.auth = "sign=" + sign + "&" + "orderno=" + orderno + "&" + "timestamp=" + timestamp
        self.proxy = "http://" + ip_port



    def process_request(self,request,spider):
        # print(self.fake.user_agent())
        request.meta['proxy'] = self.proxy
        request.headers.setdefault('Proxy-Authorization', self.auth)